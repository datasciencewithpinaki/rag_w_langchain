{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae3face-05aa-4ab5-84c7-82105e3ab5b3",
   "metadata": {},
   "source": [
    "# Use the kernel for \n",
    "`11_retreive_docs.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d68be-d844-4874-b0f4-f6ac1ee1de4e",
   "metadata": {},
   "source": [
    "# LLM Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e79047-9121-4d98-a43d-9355c7865b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4o'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = [\"gpt-3.5-turbo-instruct\", \"gpt-3.5-turbo-1106\", \"gpt-4\", \"gpt-4o\"]\n",
    "model = model_names[-1]\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdde00e2-85bf-41f2-a76b-000e10bf301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0ed351a-2812-4fd1-959e-ef737012d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"LangChain is a powerful framework designed to facilitate the development of applications that leverage large language models (LLMs). Retrieval-Augmented Generation (RAG) is a technique that combines the strengths of retrieval-based methods and generative models to produce more accurate and contextually relevant responses. Using LangChain with RAG can significantly enhance the capabilities of your application. Here are some ways to integrate LangChain with RAG:\\n\\n### 1. **Document Retrieval and Augmentation**\\n   - **Document Loaders**: Use LangChain's document loaders to ingest and preprocess documents from various sources (e.g., PDFs, web pages, databases).\\n   - **Vector Stores**: Store the preprocessed documents in a vector store (e.g., FAISS, Pinecone) to enable efficient similarity search.\\n   - **Retrieval**: Implement a retrieval mechanism to fetch relevant documents based on the user's query. This can be done using similarity search or other retrieval techniques.\\n   - **Augmentation**: Use the retrieved documents to augment the input to the generative model, providing it with additional context.\\n\\n### 2. **Combining Retrieval with Generation**\\n   - **Prompt Engineering**: Design prompts that incorporate the retrieved documents. For example, you can concatenate the user's query with the most relevant documents before passing it to the generative model.\\n   - **Chain of Thought**: Use LangChain's chain of thought capabilities to structure the interaction between retrieval and generation. This can involve multiple steps of retrieval and generation to refine the response.\\n\\n### 3. **Custom Pipelines**\\n   - **Custom Chains**: Create custom chains in LangChain that define the sequence of operations for RAG. For example, a chain might involve retrieving documents, summarizing them, and then generating a response based on the summary.\\n   - **Modular Components**: Leverage LangChain's modular components to build and customize your RAG pipeline. This includes components for retrieval, summarization, and generation.\\n\\n### 4. **Integration with External Tools**\\n   - **APIs and Services**: Integrate LangChain with external APIs and services for document retrieval and storage. For example, you can use APIs from search engines, databases, or cloud storage services.\\n   - **Custom Models**: Use custom retrieval models or fine-tuned generative models within the LangChain framework to improve the performance of your RAG system.\\n\\n### 5. **Evaluation and Feedback**\\n   - **Evaluation Metrics**: Implement evaluation metrics to assess the performance of your RAG system. This can include metrics for retrieval accuracy, generation quality, and overall user satisfaction.\\n   - **Feedback Loops**: Incorporate user feedback to continuously improve the retrieval and generation components. LangChain can facilitate the collection and integration of feedback into the system.\\n\\n### Example Workflow\\nHere's a simplified example workflow for using LangChain with RAG:\\n\\n1. **Document Ingestion**: Load documents using LangChain's document loaders.\\n2. **Vectorization**: Convert documents into vector representations and store them in a vector store.\\n3. **Query Processing**: When a user submits a query, retrieve the most relevant documents from the vector store.\\n4. **Contextual Augmentation**: Augment the user's query with the retrieved documents.\\n5. **Response Generation**: Pass the augmented query to a generative model to produce a response.\\n6. **Post-Processing**: Optionally, post-process the generated response to improve readability or relevance.\\n7. **Feedback Collection**: Collect user feedback to refine the retrieval and generation components.\\n\\nBy combining LangChain with RAG, you can create sophisticated applications that leverage the best of both retrieval and generation techniques, resulting in more accurate and contextually relevant responses.\" response_metadata={'token_usage': {'completion_tokens': 755, 'prompt_tokens': 18, 'total_tokens': 773}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_43dfabdef1', 'finish_reason': 'stop', 'logprobs': None} id='run-392bbcdc-9f6d-4ca0-895f-5238e490ea79-0'\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the ways to use langchain with rag?\"\n",
    "\n",
    "if model == \"gpt-3.5-turbo-instruct\":\n",
    "    llm = OpenAI(model_name=model, temperature=0.)\n",
    "    response = llm.invoke(query)  # response --> output as str\n",
    "    print(response)\n",
    "    \n",
    "elif model == \"gpt-3.5-turbo-1106\":\n",
    "    llm = ChatOpenAI(model_name=model, temperature=0.)\n",
    "    response = llm.invoke(query)  # response.content --> output as str\n",
    "    print(response)\n",
    "    \n",
    "elif model == \"gpt-4\":\n",
    "    llm = ChatOpenAI(model_name=model, temperature=0.)\n",
    "    response = llm.invoke(query)  # response.content --> output as str\n",
    "    print(response)\n",
    "    \n",
    "elif model == \"gpt-4o\":\n",
    "    llm = ChatOpenAI(model_name=model, temperature=0.)\n",
    "    response = llm.invoke(query)  # response.content --> output as str\n",
    "    print(response)\n",
    "    \n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c4f55f0-d0bf-45e9-a721-c677acdd9ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a powerful framework designed to facilitate the development of applications that leverage large language models (LLMs). Retrieval-Augmented Generation (RAG) is a technique that combines the strengths of retrieval-based methods and generative models to produce more accurate and contextually relevant responses. Using LangChain with RAG can significantly enhance the capabilities of your application. Here are some ways to integrate LangChain with RAG:\n",
      "\n",
      "### 1. **Document Retrieval and Augmentation**\n",
      "   - **Document Loaders**: Use LangChain's document loaders to ingest and preprocess documents from various sources (e.g., PDFs, web pages, databases).\n",
      "   - **Vector Stores**: Store the preprocessed documents in a vector store (e.g., FAISS, Pinecone) to enable efficient similarity search.\n",
      "   - **Retrieval**: Implement a retrieval mechanism to fetch relevant documents based on the user's query. This can be done using similarity search or other retrieval techniques.\n",
      "   - **Augmentation**: Use the retrieved documents to augment the input to the generative model, providing it with additional context.\n",
      "\n",
      "### 2. **Combining Retrieval with Generation**\n",
      "   - **Prompt Engineering**: Design prompts that incorporate the retrieved documents. For example, you can concatenate the user's query with the most relevant documents before passing it to the generative model.\n",
      "   - **Chain of Thought**: Use LangChain's chain of thought capabilities to structure the interaction between retrieval and generation. This can involve multiple steps of retrieval and generation to refine the response.\n",
      "\n",
      "### 3. **Custom Pipelines**\n",
      "   - **Custom Chains**: Create custom chains in LangChain that define the sequence of operations for RAG. For example, a chain might involve retrieving documents, summarizing them, and then generating a response based on the summary.\n",
      "   - **Modular Components**: Leverage LangChain's modular components to build and customize your RAG pipeline. This includes components for retrieval, summarization, and generation.\n",
      "\n",
      "### 4. **Integration with External Tools**\n",
      "   - **APIs and Services**: Integrate LangChain with external APIs and services for document retrieval and storage. For example, you can use APIs from search engines, databases, or cloud storage services.\n",
      "   - **Custom Models**: Use custom retrieval models or fine-tuned generative models within the LangChain framework to improve the performance of your RAG system.\n",
      "\n",
      "### 5. **Evaluation and Feedback**\n",
      "   - **Evaluation Metrics**: Implement evaluation metrics to assess the performance of your RAG system. This can include metrics for retrieval accuracy, generation quality, and overall user satisfaction.\n",
      "   - **Feedback Loops**: Incorporate user feedback to continuously improve the retrieval and generation components. LangChain can facilitate the collection and integration of feedback into the system.\n",
      "\n",
      "### Example Workflow\n",
      "Here's a simplified example workflow for using LangChain with RAG:\n",
      "\n",
      "1. **Document Ingestion**: Load documents using LangChain's document loaders.\n",
      "2. **Vectorization**: Convert documents into vector representations and store them in a vector store.\n",
      "3. **Query Processing**: When a user submits a query, retrieve the most relevant documents from the vector store.\n",
      "4. **Contextual Augmentation**: Augment the user's query with the retrieved documents.\n",
      "5. **Response Generation**: Pass the augmented query to a generative model to produce a response.\n",
      "6. **Post-Processing**: Optionally, post-process the generated response to improve readability or relevance.\n",
      "7. **Feedback Collection**: Collect user feedback to refine the retrieval and generation components.\n",
      "\n",
      "By combining LangChain with RAG, you can create sophisticated applications that leverage the best of both retrieval and generation techniques, resulting in more accurate and contextually relevant responses.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49008ebb-ffdc-4b8a-ba98-748261dbc90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b629bc-0411-441a-9451-325f9000abaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c0981-6cf7-4a70-98d4-e201b22cbc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c42589b-49a4-4b09-9dcf-f8a387f5b8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65cadb-1b3e-4b26-9bc0-87e000ca1810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f47e90-0ca7-418f-8e0f-f881126a2ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
